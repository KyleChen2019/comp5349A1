Format the file system with:
hdfs namenode -format

Then start all HDFS services with
$HADOOP_HOME/sbin/start-dfs.sh

	http://localhost:50070/

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/kche3935

		//stop
		$HADOOP_HOME/sbin/stop-dfs.sh




//yarn
$HADOOP_HOME/sbin/start-yarn.sh
	
	http://localhost:8088/

Create a directory on HDFS to save application logs
hdfs dfs -mkdir -p /var/log/hadoop-yarn/apps

Start a job history server so we have a web UI to access job logs:
$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver

		To stop jobhistory, run the following
		$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh stop historyserver

		To stop YARN, run the following:
		$HADOOP_HOME/sbin/stop-yarn.sh
(1)0 data node----
	delete hadoop-data hadoop-logs
	source .profile	
	recreate
(2)safe mode----
	cd /usr/local/hadoop-2.9.0
	bin/hadoop dfsadmin -safemode leave


1.xml->jar
ant

2.put file on hdfs
hdfs dfs -put ~/comp5349/lab_commons/data/partial.txt partial.txt
hdfs dfs -put ~/comp5349/lab_commons/data/a1/ALLvideos.csv out1.csv

3.runs the code :
hadoop jar userTag.jar usertag.TagDriver partial.txt userTagOutNaive1
hadoop jar userTag.jar usertag.TagDriver out1.csv out1

4.delte old output
hdfs dfs -rm -r -f out1



week6
• HDFS name node process, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:50070.
• YARN Resource Manager process, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:8088.
• MapReduce history server, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:19888
• Spark history server, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:18080
Q1:

 ssh soit-hdp-pro-1.ucc.usyd.edu.au


	hdfs-site.xml	core-site.xml
	$HADOOP_HOME/sbin/start-dfs.sh
	hdfs dfs -put ~/comp5349/lab_commons/data/place.txt /user/kche3935/place.txt
	hdfs dfs -ls /share/movie


