Format the file system with:
hdfs namenode -format

Then start all HDFS services with
$HADOOP_HOME/sbin/start-dfs.sh

	http://localhost:50070/

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/kche3935

		//stop
		$HADOOP_HOME/sbin/stop-dfs.sh




//yarn
$HADOOP_HOME/sbin/start-yarn.sh
	
	http://localhost:8088/

Create a directory on HDFS to save application logs
hdfs dfs -mkdir -p /var/log/hadoop-yarn/apps

Start a job history server so we have a web UI to access job logs:
$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver

		To stop jobhistory, run the following
		$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh stop historyserver

		To stop YARN, run the following:
		$HADOOP_HOME/sbin/stop-yarn.sh
(1)0 data node----
	delete hadoop-data hadoop-logs
	source .profile	
	recreate
(2)safe mode----
	cd /usr/local/hadoop-2.9.0
	bin/hadoop dfsadmin -safemode leave


1.xml->jar
ant

2.put file on hdfs
hdfs dfs -put ~/comp5349/lab_commons/data/partial.txt partial.txt
hdfs dfs -put ~/comp5349/lab_commons/data/a1/ALLvideos.csv out1.csv
hdfs dfs -put ~/comp5349/lab_commons/data/a1/ALLvideos.csv /user/kche3935/out1.csv


3.runs the code :
hadoop jar userTag.jar usertag.TagDriver partial.txt userTagOutNaive1
hadoop jar userTag.jar usertag.TagDriver out1.csv out1 GB US
hadoop jar userTag.jar usertag.TagDriver /user/kche3935/out1.csv out1 GB US

4.delte old output
hdfs dfs -rm -r -f out1
hdfs dfs -rm -r -f /user/kche3935/out1

-------------------------------------------------------------------------------
week6
• HDFS name node process, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:50070
• YARN Resource Manager process, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:8088.
• MapReduce history server, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:19888
• Spark history server, with a web UI:
http://soit-hdp-pro-1.ucc.usyd.edu.au:18080
Q1:

 ssh soit-hdp-pro-1.ucc.usyd.edu.au
	exit


	hdfs-site.xml	core-site.xml
	$HADOOP_HOME/sbin/start-dfs.sh
	hdfs dfs -put ~/comp5349/lab_commons/data/place.txt /user/kche3935/place.txt
	hdfs dfs -ls /share/movie





----------------------------------------------------------------------------------------
Spark part:

Start spark history server with the following command:
$SPARK_HOME/sbin/start-history-server.sh

$HADOOP_HOME/sbin/start-dfs.sh

$HADOOP_HOME/sbin/start-yarn.sh

sh soit-hdp-pro-1.ucc.usyd.edu.au
ssh soit-hdp-pro-1.ucc.usyd.edu.au:8088
http://soit-hdp-pro-1.ucc.usyd.edu.au:8088

1.xml->jar
ant

2.put file on hdfs
hdfs dfs -put ~/comp5349/lab_commons/data/week5/ week5
hdfs dfs -put ~/comp5349/lab_commons/data/week5/ /user/kche3935/week5

3.runs the code :
spark-submit --class ml.MovieLensLarge --master local[4] sparkML.jar week5/ week5-out-java/
spark-submit --class ml.MovieLensLarge --master local[4] sparkML.jar week5/ week5-out-java/
spark-submit --class ml.MovieLensLarge --master local[4] sparkML.jar / testA2/


spark-submit --class ml.MovieLensLarge --master local[4] sparkML.jar /user/kche3935/week5/ out-sparkTmp/
spark-submit --class ml.MovieLensLarge --master yarn --deploy-mode cluster --num-executors 3 sparkML.jar \
/share/movie/ week6-out-java
